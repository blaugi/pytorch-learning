{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1c5d18",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 04: Losses - Measuring How Wrong We Are\n",
    "\n",
    "Welcome to Module 04! Today you'll implement the mathematical functions that measure how wrong your model's predictions are - the essential feedback signal that enables all machine learning.\n",
    "\n",
    "## ğŸ”— Prerequisites & Progress\n",
    "**You've Built**: Tensors (data), Activations (intelligence), Layers (architecture)\n",
    "**You'll Build**: Loss functions that measure prediction quality\n",
    "**You'll Enable**: The feedback signal needed for training (Module 06: Autograd)\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Layers â†’ Losses â†’ Autograd\n",
    "(predictions) (error measurement) (learning signals)\n",
    "```\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "1. Implement MSELoss for regression problems\n",
    "2. Implement CrossEntropyLoss for classification problems\n",
    "3. Implement BinaryCrossEntropyLoss for binary classification\n",
    "4. Understand numerical stability in loss computation\n",
    "5. Test all loss functions with realistic examples\n",
    "\n",
    "Let's measure prediction quality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d8c31",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `src/04_losses/04_losses.py`\n",
    "**Building Side:** Code exports to `tinytorch.core.losses`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.losses import MSELoss, CrossEntropyLoss, BinaryCrossEntropyLoss, log_softmax  # This module\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete loss function system in one focused module\n",
    "- **Production:** Proper organization like PyTorch's torch.nn functional losses\n",
    "- **Consistency:** All loss computations and numerical stability in core.losses\n",
    "- **Integration:** Works seamlessly with layers for complete prediction-to-error workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81359ca8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“‹ Module Dependencies\n",
    "\n",
    "**Prerequisites**: Modules 01 (Tensor), 02 (Activations), and 03 (Layers) must be completed\n",
    "\n",
    "**External Dependencies**:\n",
    "- `numpy` (for numerical operations)\n",
    "\n",
    "**TinyTorch Dependencies**:\n",
    "- **Module 01 (Tensor)**: Foundation for all loss computations\n",
    "  - Used for: Input/output data structures, shape operations, element-wise operations\n",
    "  - Required: Yes - losses operate on Tensor objects\n",
    "- **Module 02 (Activations)**: Activation functions for testing\n",
    "  - Used for: ReLU for building test networks that generate realistic outputs\n",
    "  - Required: Yes - for testing loss functions with realistic predictions\n",
    "- **Module 03 (Layers)**: Layer components for testing\n",
    "  - Used for: Linear layer for testing loss functions with realistic predictions\n",
    "  - Required: Yes - for building test networks\n",
    "\n",
    "**Dependency Flow**:\n",
    "```\n",
    "Module 01 (Tensor) â†’ Module 02 (Activations) â†’ Module 03 (Layers) â†’ Module 04 (Losses) â†’ Module 05 (DataLoader) â†’ Module 06 (Autograd)\n",
    "     â†“                      â†“                         â†“                    â†“                    â†“                      â†“\n",
    "  Foundation          Nonlinearity              Architecture        Error Measurement      Data Pipelines       Gradient Flow\n",
    "```\n",
    "\n",
    "**Import Strategy**:\n",
    "This module imports directly from the TinyTorch package (`from tinytorch.core.*`).\n",
    "**Assumption**: Modules 01 (Tensor), 02 (Activations), and 03 (Layers) have been completed and exported to the package.\n",
    "If you see import errors, ensure you've run `tito export` after completing previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa2021",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.losses\n",
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "# Import from TinyTorch package (previous modules must be completed and exported)\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.activations import ReLU\n",
    "from tinytorch.core.layers import Linear\n",
    "\n",
    "# Constants for numerical stability\n",
    "EPSILON = 1e-7  # Small value to prevent log(0) and numerical instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cbed7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Introduction - What Are Loss Functions?\n",
    "\n",
    "Loss functions are the mathematical conscience of machine learning. They measure the distance between what your model predicts and what actually happened. Without loss functions, models have no way to improve - they're like athletes training without knowing their score.\n",
    "\n",
    "## ğŸ’¡ The Three Essential Loss Functions\n",
    "\n",
    "Think of loss functions as different ways to measure \"wrongness\" - each optimized for different types of problems:\n",
    "\n",
    "**MSELoss (Mean Squared Error)**: \"How far off are my continuous predictions?\"\n",
    "- Used for: Regression (predicting house prices, temperature, stock values)\n",
    "- Calculation: Average of squared differences between predictions and targets\n",
    "- Properties: Heavily penalizes large errors, smooth gradients\n",
    "\n",
    "```\n",
    "Loss Landscape for MSE:\n",
    "     Loss\n",
    "      ^\n",
    "      |\n",
    "   4  |     *\n",
    "      |    / \\\n",
    "   2  |   /   \\\n",
    "      |  /     \\\n",
    "   0  |_/_______\\\\____> Prediction Error\n",
    "      0  -2  0  +2\n",
    "\n",
    "Quadratic growth: small errors â†’ small penalty, large errors â†’ huge penalty\n",
    "```\n",
    "\n",
    "**CrossEntropyLoss**: \"How confident am I in the wrong class?\"\n",
    "- Used for: Multi-class classification (image recognition, text classification)\n",
    "- Calculation: Negative log-likelihood of correct class probability\n",
    "- Properties: Encourages confident correct predictions, punishes confident wrong ones\n",
    "\n",
    "```\n",
    "Cross-Entropy Penalty Curve:\n",
    "     Loss\n",
    "      ^\n",
    "   10 |*\n",
    "      ||\n",
    "    5 | \\\n",
    "      |  \\\n",
    "    2 |   \\\n",
    "      |    \\\n",
    "    0 |_____\\\\____> Predicted Probability of Correct Class\n",
    "      0   0.5   1.0\n",
    "\n",
    "Logarithmic: wrong confident predictions get severe penalty\n",
    "```\n",
    "\n",
    "**BinaryCrossEntropyLoss**: \"How wrong am I about yes/no decisions?\"\n",
    "- Used for: Binary classification (spam detection, medical diagnosis)\n",
    "- Calculation: Cross-entropy specialized for two classes\n",
    "- Properties: Symmetric penalty for false positives and false negatives\n",
    "\n",
    "```\n",
    "Binary Decision Boundary:\n",
    "     Target=1 (Positive)    Target=0 (Negative)\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚  Pred â†’ 1.0     â”‚  Pred â†’ 1.0     â”‚\n",
    "     â”‚  Loss â†’ 0       â”‚  Loss â†’ âˆ       â”‚\n",
    "     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "     â”‚  Pred â†’ 0.0     â”‚  Pred â†’ 0.0     â”‚\n",
    "     â”‚  Loss â†’ âˆ       â”‚  Loss â†’ 0       â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Each loss function creates a different \"error landscape\" that guides learning in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855da9a6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“ Mathematical Foundations\n",
    "\n",
    "## ğŸ“ Mean Squared Error (MSE)\n",
    "The foundation of regression, MSE measures the average squared distance between predictions and targets:\n",
    "\n",
    "```\n",
    "MSE = (1/N) * Î£(prediction_i - target_i)Â²\n",
    "```\n",
    "\n",
    "**Why square the differences?**\n",
    "- Makes all errors positive (no cancellation between positive/negative errors)\n",
    "- Heavily penalizes large errors (error of 2 becomes 4, error of 10 becomes 100)\n",
    "- Creates smooth gradients for optimization\n",
    "\n",
    "## ğŸ“ Cross-Entropy Loss\n",
    "For classification, we need to measure how wrong our probability distributions are:\n",
    "\n",
    "```\n",
    "CrossEntropy = -Î£ target_i * log(prediction_i)\n",
    "```\n",
    "\n",
    "**The Log-Sum-Exp Trick**:\n",
    "Computing softmax directly can cause numerical overflow. The log-sum-exp trick provides stability:\n",
    "```\n",
    "log_softmax(x) = x - log(Î£ exp(x_i))\n",
    "                = x - max(x) - log(Î£ exp(x_i - max(x)))\n",
    "```\n",
    "\n",
    "This prevents exp(large_number) from exploding to infinity.\n",
    "\n",
    "## ğŸ“ Binary Cross-Entropy\n",
    "A specialized case where we have only two classes:\n",
    "```\n",
    "BCE = -(target * log(prediction) + (1-target) * log(1-prediction))\n",
    "```\n",
    "\n",
    "The mathematics naturally handles both \"positive\" and \"negative\" cases in a single formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed47851",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ—ï¸ Implementation - Building Loss Functions\n",
    "\n",
    "Let's implement our loss functions with proper numerical stability and clear educational structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55d46f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Log-Softmax - The Numerically Stable Foundation\n",
    "\n",
    "Before implementing loss functions, we need a reliable way to compute log-softmax. This function is the numerically stable backbone of classification losses.\n",
    "\n",
    "### Why Log-Softmax Matters\n",
    "\n",
    "Naive softmax can explode with large numbers:\n",
    "```\n",
    "Naive approach:\n",
    "  logits = [100, 200, 300]\n",
    "  exp(300) = 1.97 Ã— 10^130  â† This breaks computers!\n",
    "\n",
    "Stable approach:\n",
    "  max_logit = 300\n",
    "  shifted = [-200, -100, 0]  â† Subtract max\n",
    "  exp(0) = 1.0  â† Manageable numbers\n",
    "```\n",
    "\n",
    "### The Log-Sum-Exp Trick Visualization\n",
    "\n",
    "```\n",
    "Original Computation:           Stable Computation:\n",
    "\n",
    "logits: [a, b, c]              logits: [a, b, c]\n",
    "   â†“                              â†“\n",
    "exp(logits)                    max_val = max(a,b,c)\n",
    "   â†“                              â†“\n",
    "sum(exp(logits))               shifted = [a-max, b-max, c-max]\n",
    "   â†“                              â†“\n",
    "log(sum)                       exp(shifted)  â† All â‰¤ 1.0\n",
    "   â†“                              â†“\n",
    "logits - log(sum)              sum(exp(shifted))\n",
    "                                  â†“\n",
    "                               log(sum) + max_val\n",
    "                                  â†“\n",
    "                               logits - (log(sum) + max_val)\n",
    "```\n",
    "\n",
    "Both give the same result, but the stable version never overflows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada2588",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "log_softmax",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_softmax(x: Tensor, dim: int = -1) -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute log-softmax with numerical stability.\n",
    "\n",
    "    TODO: Implement numerically stable log-softmax using the log-sum-exp trick\n",
    "\n",
    "    APPROACH:\n",
    "    1. Find maximum along dimension (for stability)\n",
    "    2. Subtract max from input (prevents overflow)\n",
    "    3. Compute log(sum(exp(shifted_input)))\n",
    "    4. Return input - max - log_sum_exp\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> logits = Tensor([[1.0, 2.0, 3.0], [0.1, 0.2, 0.9]])\n",
    "    >>> result = log_softmax(logits, dim=-1)\n",
    "    >>> print(result.shape)\n",
    "    (2, 3)\n",
    "\n",
    "    HINT: Use np.max(x.data, axis=dim, keepdims=True) to preserve dimensions\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    max_value = x.max(axis=dim, keepdims=True)\n",
    "    tensor_shifted = x - max_value\n",
    "    log_sum_exp = np.log(np.sum(np.exp(tensor_shifted.data), axis=dim, keepdims=True))\n",
    "    return x - max_value - log_sum_exp\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ee88e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_log_softmax",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_log_softmax():\n",
    "    \"\"\"ğŸ”¬ Test log_softmax numerical stability and correctness.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Log-Softmax...\")\n",
    "\n",
    "    # Test basic functionality\n",
    "    x = Tensor([[1.0, 2.0, 3.0], [0.1, 0.2, 0.9]])\n",
    "    result = log_softmax(x, dim=-1)\n",
    "\n",
    "    # Verify shape preservation\n",
    "    assert result.shape == x.shape, f\"Shape mismatch: expected {x.shape}, got {result.shape}\"\n",
    "\n",
    "    # Verify log-softmax properties: exp(log_softmax) should sum to 1\n",
    "    softmax_result = np.exp(result.data)\n",
    "    row_sums = np.sum(softmax_result, axis=-1)\n",
    "    assert np.allclose(row_sums, 1.0, atol=1e-6), f\"Softmax doesn't sum to 1: {row_sums}\"\n",
    "\n",
    "    # Test numerical stability with large values\n",
    "    large_x = Tensor([[100.0, 101.0, 102.0]])\n",
    "    large_result = log_softmax(large_x, dim=-1)\n",
    "    assert not np.any(np.isnan(large_result.data)), \"NaN values in result with large inputs\"\n",
    "    assert not np.any(np.isinf(large_result.data)), \"Inf values in result with large inputs\"\n",
    "\n",
    "    print(\"âœ… log_softmax works correctly with numerical stability!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3d3a8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ MSELoss - Measuring Continuous Prediction Quality\n",
    "\n",
    "Mean Squared Error is the workhorse of regression problems. It measures how far your continuous predictions are from the true values.\n",
    "\n",
    "### When to Use MSE\n",
    "\n",
    "**Perfect for:**\n",
    "- House price prediction ($200k vs $195k)\n",
    "- Temperature forecasting (25Â°C vs 23Â°C)\n",
    "- Stock price prediction ($150 vs $148)\n",
    "- Any continuous value where \"distance\" matters\n",
    "\n",
    "### How MSE Shapes Learning\n",
    "\n",
    "```\n",
    "Prediction vs Target Visualization:\n",
    "\n",
    "Target = 100\n",
    "\n",
    "Prediction: 80   90   95   100  105  110  120\n",
    "Error:     -20  -10   -5    0   +5  +10  +20\n",
    "MSE:       400  100   25    0   25  100  400\n",
    "\n",
    "Loss Curve:\n",
    "     MSE\n",
    "      ^\n",
    "  400 |*           *\n",
    "      |\n",
    "  100 | *         *\n",
    "      |  \\\n",
    "   25 |   *     *\n",
    "      |    \\\\   /\n",
    "    0 |_____*_____> Prediction\n",
    "       80   100   120\n",
    "\n",
    "Quadratic penalty: Large errors are MUCH more costly than small errors\n",
    "```\n",
    "\n",
    "### Why Square the Errors?\n",
    "\n",
    "1. **Positive penalties**: (-10)Â² = 100, same as (+10)Â² = 100\n",
    "2. **Heavy punishment for large errors**: Error of 20 â†’ penalty of 400\n",
    "3. **Smooth gradients**: Quadratic function has nice derivatives for optimization\n",
    "4. **Statistical foundation**: Maximum likelihood for Gaussian noise\n",
    "\n",
    "### MSE vs Other Regression Losses\n",
    "\n",
    "```\n",
    "Error Sensitivity Comparison:\n",
    "\n",
    " Error:   -10    -5     0     +5    +10\n",
    " MSE:     100    25     0     25    100  â† Quadratic growth\n",
    " MAE:      10     5     0      5     10  â† Linear growth\n",
    " Huber:    50    12.5   0    12.5    50  â† Hybrid approach\n",
    "\n",
    " MSE: More sensitive to outliers\n",
    " MAE: More robust to outliers\n",
    " Huber: Best of both worlds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd743336",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "mse_loss",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class MSELoss:\n",
    "    \"\"\"Mean Squared Error loss for regression tasks.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize MSE loss function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute mean squared error between predictions and targets.\n",
    "\n",
    "        TODO: Implement MSE loss calculation\n",
    "\n",
    "        APPROACH:\n",
    "        1. Compute difference: predictions - targets\n",
    "        2. Square the differences: diffÂ²\n",
    "        3. Take mean across all elements\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> loss_fn = MSELoss()\n",
    "        >>> predictions = Tensor([1.0, 2.0, 3.0])\n",
    "        >>> targets = Tensor([1.5, 2.5, 2.8])\n",
    "        >>> loss = loss_fn(predictions, targets)\n",
    "        >>> print(f\"MSE Loss: {loss.data:.4f}\")\n",
    "        MSE Loss: 0.1467\n",
    "\n",
    "        HINTS:\n",
    "        - Use (predictions.data - targets.data) for element-wise difference\n",
    "        - Square with **2 or np.power(diff, 2)\n",
    "        - Use np.mean() to average over all elements\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Step 1: Compute element-wise difference\n",
    "        diff = predictions - targets\n",
    "        square = np.power(diff.data, 2)\n",
    "        mse = Tensor(np.mean(square))\n",
    "\n",
    "        return mse\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Allows the loss function to be called like a function.\"\"\"\n",
    "        return self.forward(predictions, targets)\n",
    "\n",
    "    def backward(self) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute gradients (implemented in Module 06: Autograd).\n",
    "\n",
    "        For now, this is a stub that students can ignore.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc67cc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_mse_loss",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_mse_loss():\n",
    "    \"\"\"ğŸ”¬ Test MSELoss implementation and properties.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: MSE Loss...\")\n",
    "\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Test perfect predictions (loss should be 0)\n",
    "    predictions = Tensor([1.0, 2.0, 3.0])\n",
    "    targets = Tensor([1.0, 2.0, 3.0])\n",
    "    perfect_loss = loss_fn.forward(predictions, targets)\n",
    "    assert np.allclose(perfect_loss.data, 0.0, atol=EPSILON), f\"Perfect predictions should have 0 loss, got {perfect_loss.data}\"\n",
    "\n",
    "    # Test known case\n",
    "    predictions = Tensor([1.0, 2.0, 3.0])\n",
    "    targets = Tensor([1.5, 2.5, 2.8])\n",
    "    loss = loss_fn.forward(predictions, targets)\n",
    "\n",
    "    # Manual calculation: ((1-1.5)Â² + (2-2.5)Â² + (3-2.8)Â²) / 3 = (0.25 + 0.25 + 0.04) / 3 = 0.18\n",
    "    expected_loss = (0.25 + 0.25 + 0.04) / 3\n",
    "    assert np.allclose(loss.data, expected_loss, atol=1e-6), f\"Expected {expected_loss}, got {loss.data}\"\n",
    "\n",
    "    # Test that loss is always non-negative\n",
    "    random_pred = Tensor(np.random.randn(10))\n",
    "    random_target = Tensor(np.random.randn(10))\n",
    "    random_loss = loss_fn.forward(random_pred, random_target)\n",
    "    assert random_loss.data >= 0, f\"MSE loss should be non-negative, got {random_loss.data}\"\n",
    "\n",
    "    print(\"âœ… MSELoss works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_mse_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead5a38",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ CrossEntropyLoss - Measuring Classification Confidence\n",
    "\n",
    "Cross-entropy loss is the gold standard for multi-class classification. It measures how wrong your probability predictions are and heavily penalizes confident mistakes.\n",
    "\n",
    "### When to Use Cross-Entropy\n",
    "\n",
    "**Perfect for:**\n",
    "- Image classification (cat, dog, bird)\n",
    "- Text classification (spam, ham, promotion)\n",
    "- Language modeling (next word prediction)\n",
    "- Any problem with mutually exclusive classes\n",
    "\n",
    "### Understanding Cross-Entropy Through Examples\n",
    "\n",
    "```\n",
    "Scenario: Image Classification (3 classes: cat, dog, bird)\n",
    "\n",
    "Case 1: Correct and Confident\n",
    "Model Output (logits): [5.0, 1.0, 0.1]  â† Very confident about \"cat\"\n",
    "After Softmax:        [0.95, 0.047, 0.003]\n",
    "True Label:           cat (class 0)\n",
    "Loss: -log(0.95) = 0.05  â† Very low loss âœ…\n",
    "\n",
    "Case 2: Correct but Uncertain\n",
    "Model Output:         [1.1, 1.0, 0.9]  â† Uncertain between classes\n",
    "After Softmax:        [0.4, 0.33, 0.27]\n",
    "True Label:           cat (class 0)\n",
    "Loss: -log(0.4) = 0.92  â† Higher loss (uncertainty penalized)\n",
    "\n",
    "Case 3: Wrong and Confident\n",
    "Model Output:         [0.1, 5.0, 1.0]  â† Very confident about \"dog\"\n",
    "After Softmax:        [0.003, 0.95, 0.047]\n",
    "True Label:           cat (class 0)\n",
    "Loss: -log(0.003) = 5.8  â† Very high loss âŒ\n",
    "```\n",
    "\n",
    "### Cross-Entropy's Learning Signal\n",
    "\n",
    "```\n",
    "What Cross-Entropy Teaches the Model:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Prediction      â”‚ True Label      â”‚ Learning Signal           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Confident âœ…    â”‚ Correct âœ…      â”‚ \"Keep doing this\"         â”‚\n",
    "â”‚ Uncertain âš ï¸    â”‚ Correct âœ…      â”‚ \"Be more confident\"       â”‚\n",
    "â”‚ Confident âŒ    â”‚ Wrong âŒ        â”‚ \"STOP! Change everything\" â”‚\n",
    "â”‚ Uncertain âš ï¸    â”‚ Wrong âŒ        â”‚ \"Learn the right answer\"  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Loss Landscape by Confidence:\n",
    "     Loss\n",
    "      ^\n",
    "    5 |*\n",
    "      ||\n",
    "    3 | *\n",
    "      |  \\\n",
    "    1 |   *\n",
    "      |    \\\\\n",
    "    0 |______**____> Predicted Probability (correct class)\n",
    "      0   0.5   1.0\n",
    "\n",
    "Message: \"Be confident when you're right!\"\n",
    "```\n",
    "\n",
    "### Why Cross-Entropy Works So Well\n",
    "\n",
    "1. **Probabilistic interpretation**: Measures quality of probability distributions\n",
    "2. **Strong gradients**: Large penalty for confident mistakes drives fast learning\n",
    "3. **Smooth optimization**: Log function provides nice gradients\n",
    "4. **Information theory**: Minimizes \"surprise\" about correct answers\n",
    "\n",
    "### Multi-Class vs Binary Classification\n",
    "\n",
    "```\n",
    "Multi-Class (3+ classes):          Binary (2 classes):\n",
    "\n",
    "Classes: [cat, dog, bird]         Classes: [spam, not_spam]\n",
    "Output:  [0.7, 0.2, 0.1]         Output:  0.8 (spam probability)\n",
    "Must sum to 1.0 âœ…               Must be between 0 and 1 âœ…\n",
    "Uses: CrossEntropyLoss            Uses: BinaryCrossEntropyLoss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade152f",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "cross_entropy_loss",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"Cross-entropy loss for multi-class classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize cross-entropy loss function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, logits: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss between logits and target class indices.\n",
    "\n",
    "        TODO: Implement cross-entropy loss with numerical stability\n",
    "\n",
    "        APPROACH:\n",
    "        1. Compute log-softmax of logits (numerically stable)\n",
    "        2. Select log-probabilities for correct classes\n",
    "        3. Return negative mean of selected log-probabilities\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> loss_fn = CrossEntropyLoss()\n",
    "        >>> logits = Tensor([[2.0, 1.0, 0.1], [0.5, 1.5, 0.8]])  # 2 samples, 3 classes\n",
    "        >>> targets = Tensor([0, 1])  # First sample is class 0, second is class 1\n",
    "        >>> loss = loss_fn(logits, targets)\n",
    "        >>> print(f\"Cross-Entropy Loss: {loss.data:.4f}\")\n",
    "\n",
    "        HINTS:\n",
    "        - Use log_softmax() for numerical stability\n",
    "        - targets.data.astype(int) ensures integer indices\n",
    "        - Use np.arange(batch_size) for row indexing: log_probs[np.arange(batch_size), targets]\n",
    "        - Return negative mean: -np.mean(selected_log_probs)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        log_probs= log_softmax(logits, dim=-1)\n",
    "\n",
    "        batch_size = logits.shape[0]\n",
    "        target_indices = targets.data.astype(int)\n",
    "        selected_log_probs = log_probs.data[np.arange(batch_size), target_indices]\n",
    "        cross_entropy = -np.mean(selected_log_probs)\n",
    "\n",
    "        return Tensor(cross_entropy)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def __call__(self, logits: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Allows the loss function to be called like a function.\"\"\"\n",
    "        return self.forward(logits, targets)\n",
    "\n",
    "    def backward(self) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute gradients (implemented in Module 06: Autograd).\n",
    "\n",
    "        For now, this is a stub that students can ignore.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fb37c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_cross_entropy_loss",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_cross_entropy_loss():\n",
    "    \"\"\"ğŸ”¬ Test CrossEntropyLoss implementation and properties.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Cross-Entropy Loss...\")\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    # Test perfect predictions (should have very low loss)\n",
    "    perfect_logits = Tensor([[10.0, -10.0, -10.0], [-10.0, 10.0, -10.0]])  # Very confident predictions\n",
    "    targets = Tensor([0, 1])  # Matches the confident predictions\n",
    "    perfect_loss = loss_fn.forward(perfect_logits, targets)\n",
    "    assert perfect_loss.data < 0.01, f\"Perfect predictions should have very low loss, got {perfect_loss.data}\"\n",
    "\n",
    "    # Test uniform predictions (should have loss â‰ˆ log(num_classes))\n",
    "    uniform_logits = Tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]])  # Equal probabilities\n",
    "    uniform_targets = Tensor([0, 1])\n",
    "    uniform_loss = loss_fn.forward(uniform_logits, uniform_targets)\n",
    "    expected_uniform_loss = np.log(3)  # log(3) â‰ˆ 1.099 for 3 classes\n",
    "    assert np.allclose(uniform_loss.data, expected_uniform_loss, atol=0.1), f\"Uniform predictions should have loss â‰ˆ log(3) = {expected_uniform_loss:.3f}, got {uniform_loss.data:.3f}\"\n",
    "\n",
    "    # Test that wrong confident predictions have high loss\n",
    "    wrong_logits = Tensor([[10.0, -10.0, -10.0], [-10.0, -10.0, 10.0]])  # Confident but wrong\n",
    "    wrong_targets = Tensor([1, 1])  # Opposite of confident predictions\n",
    "    wrong_loss = loss_fn.forward(wrong_logits, wrong_targets)\n",
    "    assert wrong_loss.data > 5.0, f\"Wrong confident predictions should have high loss, got {wrong_loss.data}\"\n",
    "\n",
    "    # Test numerical stability with large logits\n",
    "    large_logits = Tensor([[100.0, 50.0, 25.0]])\n",
    "    large_targets = Tensor([0])\n",
    "    large_loss = loss_fn.forward(large_logits, large_targets)\n",
    "    assert not np.isnan(large_loss.data), \"Loss should not be NaN with large logits\"\n",
    "    assert not np.isinf(large_loss.data), \"Loss should not be infinite with large logits\"\n",
    "\n",
    "    print(\"âœ… CrossEntropyLoss works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fb253",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ BinaryCrossEntropyLoss - Measuring Yes/No Decision Quality\n",
    "\n",
    "Binary Cross-Entropy is specialized for yes/no decisions. It's like regular cross-entropy but optimized for the special case of exactly two classes.\n",
    "\n",
    "### When to Use Binary Cross-Entropy\n",
    "\n",
    "**Perfect for:**\n",
    "- Spam detection (spam vs not spam)\n",
    "- Medical diagnosis (disease vs healthy)\n",
    "- Fraud detection (fraud vs legitimate)\n",
    "- Content moderation (toxic vs safe)\n",
    "- Any two-class decision problem\n",
    "\n",
    "### Understanding Binary Cross-Entropy\n",
    "\n",
    "```\n",
    "Binary Classification Decision Matrix:\n",
    "\n",
    "                 TRUE LABEL\n",
    "              Positive  Negative\n",
    "PREDICTED  P    TP       FP     â† Model says \"Yes\"\n",
    "           N    FN       TN     â† Model says \"No\"\n",
    "\n",
    "BCE Loss for each quadrant:\n",
    "- True Positive (TP): -log(prediction)    â† Reward confident correct \"Yes\"\n",
    "- False Positive (FP): -log(1-prediction) â† Punish confident wrong \"Yes\"\n",
    "- False Negative (FN): -log(prediction)   â† Punish confident wrong \"No\"\n",
    "- True Negative (TN): -log(1-prediction)  â† Reward confident correct \"No\"\n",
    "```\n",
    "\n",
    "### Binary Cross-Entropy Behavior Examples\n",
    "\n",
    "```\n",
    "Scenario: Spam Detection\n",
    "\n",
    "Case 1: Perfect Spam Detection\n",
    "Email: \"Buy now! 50% off! Limited time!\"\n",
    "Model Prediction: 0.99 (99% spam probability)\n",
    "True Label: 1 (actually spam)\n",
    "Loss: -log(0.99) = 0.01  â† Very low loss âœ…\n",
    "\n",
    "Case 2: Uncertain About Spam\n",
    "Email: \"Meeting rescheduled to 2pm\"\n",
    "Model Prediction: 0.51 (slightly thinks spam)\n",
    "True Label: 0 (actually not spam)\n",
    "Loss: -log(1-0.51) = -log(0.49) = 0.71  â† Moderate loss\n",
    "\n",
    "Case 3: Confident Wrong Prediction\n",
    "Email: \"Hi mom, how are you?\"\n",
    "Model Prediction: 0.95 (very confident spam)\n",
    "True Label: 0 (actually not spam)\n",
    "Loss: -log(1-0.95) = -log(0.05) = 3.0  â† High loss âŒ\n",
    "```\n",
    "\n",
    "### Binary vs Multi-Class Cross-Entropy\n",
    "\n",
    "```\n",
    "Binary Cross-Entropy:              Regular Cross-Entropy:\n",
    "\n",
    "Single probability output         Probability distribution output\n",
    "Predict: 0.8 (spam prob)         Predict: [0.1, 0.8, 0.1] (3 classes)\n",
    "Target: 1.0 (is spam)            Target: 1 (class index)\n",
    "\n",
    "Formula:                         Formula:\n",
    "-[y*log(p) + (1-y)*log(1-p)]    -log(p[target_class])\n",
    "\n",
    "Handles class imbalance well     Assumes balanced classes\n",
    "Optimized for 2-class case      General for N classes\n",
    "```\n",
    "\n",
    "### Why Binary Cross-Entropy is Special\n",
    "\n",
    "1. **Symmetric penalties**: False positives and false negatives treated equally\n",
    "2. **Probability calibration**: Output directly interpretable as probability\n",
    "3. **Efficient computation**: Simpler than full softmax for binary cases\n",
    "4. **Medical-grade**: Well-suited for safety-critical binary decisions\n",
    "\n",
    "### Loss Landscape Visualization\n",
    "\n",
    "```\n",
    "Binary Cross-Entropy Loss Surface:\n",
    "\n",
    "     Loss\n",
    "      ^\n",
    "   10 |*                    *     â† Wrong confident predictions\n",
    "      ||\n",
    "    5 | *                 *\n",
    "      |  \\\\               /\n",
    "    2 |   *             *          â† Uncertain predictions\n",
    "      |    \\\\           /\n",
    "    0 |_____*_______*_____> Prediction\n",
    "      0    0.2     0.8    1.0\n",
    "\n",
    "      Target = 1.0 (positive class)\n",
    "\n",
    "Message: \"Be confident about positive class, uncertain is okay,\n",
    "         but don't be confident about wrong class!\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ffa5c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "binary_cross_entropy_loss",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class BinaryCrossEntropyLoss:\n",
    "    \"\"\"Binary cross-entropy loss for binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize binary cross-entropy loss function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "\n",
    "        TODO: Implement binary cross-entropy with numerical stability\n",
    "\n",
    "        APPROACH:\n",
    "        1. Clamp predictions to avoid log(0) and log(1)\n",
    "        2. Compute: -(targets * log(predictions) + (1-targets) * log(1-predictions))\n",
    "        3. Return mean across all samples\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> loss_fn = BinaryCrossEntropyLoss()\n",
    "        >>> predictions = Tensor([0.9, 0.1, 0.7, 0.3])  # Probabilities between 0 and 1\n",
    "        >>> targets = Tensor([1.0, 0.0, 1.0, 0.0])      # Binary labels\n",
    "        >>> loss = loss_fn(predictions, targets)\n",
    "        >>> print(f\"Binary Cross-Entropy Loss: {loss.data:.4f}\")\n",
    "\n",
    "        HINTS:\n",
    "        - Use np.clip(predictions.data, 1e-7, 1-1e-7) to prevent log(0)\n",
    "        - Binary cross-entropy: -(targets * log(preds) + (1-targets) * log(1-preds))\n",
    "        - Use np.mean() to average over all samples\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Step 1: Clamp predictions to avoid numerical issues with log(0) and log(1)\n",
    "        eps = EPSILON\n",
    "        clamped_preds = np.clip(predictions.data, eps, 1 - eps)\n",
    "\n",
    "        # Step 2: Compute binary cross-entropy\n",
    "        # BCE = -(targets * log(preds) + (1-targets) * log(1-preds))\n",
    "        bce_1 = targets.data * np.log(clamped_preds.data)\n",
    "        bce_2 = (np.array(1) - targets.data) * (np.log(np.array(1) - clamped_preds.data))\n",
    "        bce_per_sample = -(bce_1 + bce_2)\n",
    "\n",
    "        # Step 3: Return mean across all samples\n",
    "        bce_loss = np.mean(bce_per_sample)\n",
    "\n",
    "        return Tensor(bce_loss)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Allows the loss function to be called like a function.\"\"\"\n",
    "        return self.forward(predictions, targets)\n",
    "\n",
    "    def backward(self) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute gradients (implemented in Module 06: Autograd).\n",
    "\n",
    "        For now, this is a stub that students can ignore.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d666e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_binary_cross_entropy_loss",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_binary_cross_entropy_loss():\n",
    "    \"\"\"ğŸ”¬ Test BinaryCrossEntropyLoss implementation and properties.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Binary Cross-Entropy Loss...\")\n",
    "\n",
    "    loss_fn = BinaryCrossEntropyLoss()\n",
    "\n",
    "    # Test perfect predictions\n",
    "    perfect_predictions = Tensor([0.9999, 0.0001, 0.9999, 0.0001])\n",
    "    targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    perfect_loss = loss_fn.forward(perfect_predictions, targets)\n",
    "    assert perfect_loss.data < 0.01, f\"Perfect predictions should have very low loss, got {perfect_loss.data}\"\n",
    "\n",
    "    # Test worst predictions\n",
    "    worst_predictions = Tensor([0.0001, 0.9999, 0.0001, 0.9999])\n",
    "    worst_targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    worst_loss = loss_fn.forward(worst_predictions, worst_targets)\n",
    "    assert worst_loss.data > 5.0, f\"Worst predictions should have high loss, got {worst_loss.data}\"\n",
    "\n",
    "    # Test uniform predictions (probability = 0.5)\n",
    "    uniform_predictions = Tensor([0.5, 0.5, 0.5, 0.5])\n",
    "    uniform_targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    uniform_loss = loss_fn.forward(uniform_predictions, uniform_targets)\n",
    "    expected_uniform = -np.log(0.5)  # Should be about 0.693\n",
    "    assert np.allclose(uniform_loss.data, expected_uniform, atol=0.01), f\"Uniform predictions should have loss â‰ˆ {expected_uniform:.3f}, got {uniform_loss.data:.3f}\"\n",
    "\n",
    "    # Test numerical stability at boundaries\n",
    "    boundary_predictions = Tensor([0.0, 1.0, 0.0, 1.0])\n",
    "    boundary_targets = Tensor([0.0, 1.0, 1.0, 0.0])\n",
    "    boundary_loss = loss_fn.forward(boundary_predictions, boundary_targets)\n",
    "    assert not np.isnan(boundary_loss.data), \"Loss should not be NaN at boundaries\"\n",
    "    assert not np.isinf(boundary_loss.data), \"Loss should not be infinite at boundaries\"\n",
    "\n",
    "    print(\"âœ… BinaryCrossEntropyLoss works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_binary_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22692055",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Integration - Bringing It Together\n",
    "\n",
    "Now let's test how our loss functions work together with real data scenarios and explore their behavior with different types of predictions.\n",
    "\n",
    "## ğŸ”§ Real-World Loss Function Usage Patterns\n",
    "\n",
    "Understanding when and why to use each loss function is crucial for ML engineering success:\n",
    "\n",
    "```\n",
    "Problem Type Decision Tree:\n",
    "\n",
    "What are you predicting?\n",
    "         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”\n",
    "    â”‚         â”‚\n",
    "Continuous   Categorical\n",
    " Values       Classes\n",
    "    â”‚         â”‚\n",
    "    â”‚    â”Œâ”€â”€â”€â”¼â”€â”€â”€â”\n",
    "    â”‚    â”‚       â”‚\n",
    "    â”‚   2 Classes  3+ Classes\n",
    "    â”‚       â”‚       â”‚\n",
    " MSELoss   BCE Loss  CE Loss\n",
    "\n",
    "Examples:\n",
    "MSE: House prices, temperature, stock values\n",
    "BCE: Spam detection, fraud detection, medical diagnosis\n",
    "CE:  Image classification, language modeling, multiclass text classification\n",
    "```\n",
    "\n",
    "## ğŸ”§ Loss Function Behavior Comparison\n",
    "\n",
    "Each loss function creates different learning pressures on your model:\n",
    "\n",
    "```\n",
    "Error Sensitivity Comparison:\n",
    "\n",
    "Small Error (0.1):     Medium Error (0.5):     Large Error (2.0):\n",
    "\n",
    "MSE:     0.01         MSE:     0.25           MSE:     4.0\n",
    "BCE:     0.11         BCE:     0.69           BCE:     âˆ (clips to large)\n",
    "CE:      0.11         CE:      0.69           CE:      âˆ (clips to large)\n",
    "\n",
    "MSE: Quadratic growth, manageable with outliers\n",
    "BCE/CE: Logarithmic growth, explodes with confident wrong predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9990399",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss_comparison",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_loss_behaviors():\n",
    "    \"\"\"\n",
    "    ğŸ“Š Compare how different loss functions behave with various prediction patterns.\n",
    "\n",
    "    This helps students understand when to use each loss function.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Analysis: Loss Function Behavior Comparison...\")\n",
    "\n",
    "    # Initialize loss functions\n",
    "    mse_loss = MSELoss()\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "    print(\"\\n1. Regression Scenario (House Price Prediction)\")\n",
    "    print(\"   Predictions: [200k, 250k, 300k], Targets: [195k, 260k, 290k]\")\n",
    "    house_pred = Tensor([200.0, 250.0, 300.0])  # In thousands\n",
    "    house_target = Tensor([195.0, 260.0, 290.0])\n",
    "    mse = mse_loss.forward(house_pred, house_target)\n",
    "    print(f\"   MSE Loss: {mse.data:.2f} (thousandÂ²)\")\n",
    "\n",
    "    print(\"\\n2. Multi-Class Classification (Image Recognition)\")\n",
    "    print(\"   Classes: [cat, dog, bird], Predicted: confident about cat, uncertain about dog\")\n",
    "    # Logits: [2.0, 0.5, 0.1] suggests model is most confident about class 0 (cat)\n",
    "    image_logits = Tensor([[2.0, 0.5, 0.1], [0.3, 1.8, 0.2]])  # Two samples\n",
    "    image_targets = Tensor([0, 1])  # First is cat (0), second is dog (1)\n",
    "    ce = ce_loss.forward(image_logits, image_targets)\n",
    "    print(f\"   Cross-Entropy Loss: {ce.data:.3f}\")\n",
    "\n",
    "    print(\"\\n3. Binary Classification (Spam Detection)\")\n",
    "    print(\"   Predictions: [0.9, 0.1, 0.7, 0.3] (spam probabilities)\")\n",
    "    spam_pred = Tensor([0.9, 0.1, 0.7, 0.3])\n",
    "    spam_target = Tensor([1.0, 0.0, 1.0, 0.0])  # 1=spam, 0=not spam\n",
    "    bce = bce_loss.forward(spam_pred, spam_target)\n",
    "    print(f\"   Binary Cross-Entropy Loss: {bce.data:.3f}\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key Insights:\")\n",
    "    print(\"   - MSE penalizes large errors heavily (good for continuous values)\")\n",
    "    print(\"   - Cross-Entropy encourages confident correct predictions\")\n",
    "    print(\"   - Binary Cross-Entropy balances false positives and negatives\")\n",
    "\n",
    "    return mse.data, ce.data, bce.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0214a2f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss_sensitivity",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_loss_sensitivity():\n",
    "    \"\"\"\n",
    "    ğŸ“Š Analyze how sensitive each loss function is to prediction errors.\n",
    "\n",
    "    This demonstrates the different error landscapes created by each loss.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š Analysis: Loss Function Sensitivity to Errors...\")\n",
    "\n",
    "    # Create a range of prediction errors for analysis\n",
    "    true_value = 1.0\n",
    "    predictions = np.linspace(0.1, 1.9, 50)  # From 0.1 to 1.9\n",
    "\n",
    "    # Initialize loss functions\n",
    "    mse_loss = MSELoss()\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "    mse_losses = []\n",
    "    bce_losses = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        # MSE analysis\n",
    "        pred_tensor = Tensor([pred])\n",
    "        target_tensor = Tensor([true_value])\n",
    "        mse = mse_loss.forward(pred_tensor, target_tensor)\n",
    "        mse_losses.append(mse.data)\n",
    "\n",
    "        # BCE analysis (clamp prediction to valid probability range)\n",
    "        clamped_pred = max(0.01, min(0.99, pred))\n",
    "        bce_pred_tensor = Tensor([clamped_pred])\n",
    "        bce_target_tensor = Tensor([1.0])  # Target is \"positive class\"\n",
    "        bce = bce_loss.forward(bce_pred_tensor, bce_target_tensor)\n",
    "        bce_losses.append(bce.data)\n",
    "\n",
    "    # Find minimum losses\n",
    "    min_mse_idx = np.argmin(mse_losses)\n",
    "    min_bce_idx = np.argmin(bce_losses)\n",
    "\n",
    "    print(f\"MSE Loss:\")\n",
    "    print(f\"  Minimum at prediction = {predictions[min_mse_idx]:.2f}, loss = {mse_losses[min_mse_idx]:.4f}\")\n",
    "    print(f\"  At prediction = 0.5: loss = {mse_losses[24]:.4f}\")  # Middle of range\n",
    "    print(f\"  At prediction = 0.1: loss = {mse_losses[0]:.4f}\")\n",
    "\n",
    "    print(f\"\\nBinary Cross-Entropy Loss:\")\n",
    "    print(f\"  Minimum at prediction = {predictions[min_bce_idx]:.2f}, loss = {bce_losses[min_bce_idx]:.4f}\")\n",
    "    print(f\"  At prediction = 0.5: loss = {bce_losses[24]:.4f}\")\n",
    "    print(f\"  At prediction = 0.1: loss = {bce_losses[0]:.4f}\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Sensitivity Insights:\")\n",
    "    print(\"   - MSE grows quadratically with error distance\")\n",
    "    print(\"   - BCE grows logarithmically, heavily penalizing wrong confident predictions\")\n",
    "    print(\"   - Both encourage correct predictions but with different curvatures\")\n",
    "\n",
    "# Run integration analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_loss_behaviors()\n",
    "    analyze_loss_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f635ef",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Systems Analysis - Understanding Loss Function Performance\n",
    "\n",
    "Loss functions seem simple, but they have important computational and numerical properties that affect training performance. Let's analyze the systems aspects.\n",
    "\n",
    "## ğŸ“Š Computational Complexity Analysis\n",
    "\n",
    "Different loss functions have different computational costs, especially at scale:\n",
    "\n",
    "```\n",
    "Computational Cost Comparison (Batch Size B, Classes C):\n",
    "\n",
    "MSELoss:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Operation      â”‚ Complexity     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Subtraction    â”‚ O(B)           â”‚\n",
    "â”‚ Squaring       â”‚ O(B)           â”‚\n",
    "â”‚ Mean           â”‚ O(B)           â”‚\n",
    "â”‚ Total          â”‚ O(B)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "CrossEntropyLoss:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Operation      â”‚ Complexity     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Max (stability)â”‚ O(B*C)         â”‚\n",
    "â”‚ Exponential    â”‚ O(B*C)         â”‚\n",
    "â”‚ Sum            â”‚ O(B*C)         â”‚\n",
    "â”‚ Log            â”‚ O(B)           â”‚\n",
    "â”‚ Indexing       â”‚ O(B)           â”‚\n",
    "â”‚ Total          â”‚ O(B*C)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Cross-entropy is C times more expensive than MSE!\n",
    "For ImageNet (C=1000), CE is 1000x more expensive than MSE.\n",
    "```\n",
    "\n",
    "## ğŸ“Š Memory Layout and Access Patterns\n",
    "\n",
    "```\n",
    "Memory Usage Patterns:\n",
    "\n",
    "MSE Forward Pass:              CE Forward Pass:\n",
    "\n",
    "Input:  [B] predictions       Input:  [B, C] logits\n",
    "       â”‚                             â”‚\n",
    "       â”‚ subtract                    â”‚ subtract max\n",
    "       v                             v\n",
    "Temp:  [B] differences        Temp1: [B, C] shifted\n",
    "       â”‚                             â”‚\n",
    "       â”‚ square                      â”‚ exponential\n",
    "       v                             v\n",
    "Temp:  [B] squared            Temp2: [B, C] exp_vals\n",
    "       â”‚                             â”‚\n",
    "       â”‚ mean                        â”‚ sum along C\n",
    "       v                             v\n",
    "Output: [1] scalar            Temp3: [B] sums\n",
    "                                     â”‚\n",
    "Memory: 3*B*sizeof(float)            â”‚ log + index\n",
    "                                     v\n",
    "                              Output: [1] scalar\n",
    "\n",
    "                              Memory: (3*B*C + 2*B)*sizeof(float)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91321c5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze_numerical_stability",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_numerical_stability():\n",
    "    \"\"\"\n",
    "    ğŸ“Š Demonstrate why numerical stability matters in loss computation.\n",
    "\n",
    "    Shows the difference between naive and stable implementations.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Analysis: Numerical Stability in Loss Functions...\")\n",
    "\n",
    "    # Test with increasingly large logits\n",
    "    test_cases = [\n",
    "        (\"Small logits\", [1.0, 2.0, 3.0]),\n",
    "        (\"Medium logits\", [10.0, 20.0, 30.0]),\n",
    "        (\"Large logits\", [100.0, 200.0, 300.0]),\n",
    "        (\"Very large logits\", [500.0, 600.0, 700.0])\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLog-Softmax Stability Test:\")\n",
    "    print(\"Case                 | Max Input | Log-Softmax Min | Numerically Stable?\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for case_name, logits in test_cases:\n",
    "        x = Tensor([logits])\n",
    "\n",
    "        # Our stable implementation\n",
    "        stable_result = log_softmax(x, dim=-1)\n",
    "\n",
    "        max_input = np.max(logits)\n",
    "        min_output = np.min(stable_result.data)\n",
    "        is_stable = not (np.any(np.isnan(stable_result.data)) or np.any(np.isinf(stable_result.data)))\n",
    "\n",
    "        print(f\"{case_name:20} | {max_input:8.0f} | {min_output:15.3f} | {'âœ… Yes' if is_stable else 'âŒ No'}\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Key Insight: Log-sum-exp trick prevents overflow\")\n",
    "    print(\"   Without it: exp(700) would cause overflow in standard softmax\")\n",
    "    print(\"   With it: We can handle arbitrarily large logits safely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d2fb6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze_loss_memory",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_loss_memory():\n",
    "    \"\"\"\n",
    "    ğŸ“Š Analyze memory usage patterns of different loss functions.\n",
    "\n",
    "    Understanding memory helps with batch size decisions.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š Analysis: Loss Function Memory Usage...\")\n",
    "\n",
    "    batch_sizes = [32, 128, 512, 1024]\n",
    "    num_classes = 1000  # Like ImageNet\n",
    "\n",
    "    print(\"\\nMemory Usage by Batch Size:\")\n",
    "    print(\"Batch Size | MSE (MB) | CrossEntropy (MB) | BCE (MB) | Notes\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        # Memory calculations (assuming float32 = 4 bytes)\n",
    "        bytes_per_float = 4\n",
    "\n",
    "        # MSE: predictions + targets (both same size as output)\n",
    "        mse_elements = batch_size * 1  # Regression usually has 1 output\n",
    "        mse_memory = mse_elements * bytes_per_float * 2 / 1e6  # Convert to MB\n",
    "\n",
    "        # CrossEntropy: logits + targets + softmax + log_softmax\n",
    "        ce_logits = batch_size * num_classes\n",
    "        ce_targets = batch_size * 1  # Target indices\n",
    "        ce_softmax = batch_size * num_classes  # Intermediate softmax\n",
    "        ce_total_elements = ce_logits + ce_targets + ce_softmax\n",
    "        ce_memory = ce_total_elements * bytes_per_float / 1e6\n",
    "\n",
    "        # BCE: predictions + targets (binary, so smaller)\n",
    "        bce_elements = batch_size * 1\n",
    "        bce_memory = bce_elements * bytes_per_float * 2 / 1e6\n",
    "\n",
    "        notes = \"Linear scaling\" if batch_size == 32 else f\"{batch_size//32}Ã— first\"\n",
    "\n",
    "        print(f\"{batch_size:10} | {mse_memory:8.2f} | {ce_memory:13.2f} | {bce_memory:7.2f} | {notes}\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Memory Insights:\")\n",
    "    print(\"   - CrossEntropy dominates due to large vocabulary (num_classes)\")\n",
    "    print(\"   - Memory scales linearly with batch size\")\n",
    "    print(\"   - Intermediate activations (softmax) double CE memory\")\n",
    "    print(f\"   - For batch=1024, CE needs {ce_memory:.1f}MB just for loss computation\")\n",
    "\n",
    "# Run systems analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_numerical_stability()\n",
    "    analyze_loss_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5924f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Production Context - How Loss Functions Scale\n",
    "\n",
    "Understanding how loss functions behave in production helps make informed engineering decisions about model architecture and training strategies.\n",
    "\n",
    "## ğŸ“Š Loss Function Scaling Challenges\n",
    "\n",
    "As models grow larger, loss function bottlenecks become critical:\n",
    "\n",
    "```\n",
    "Scaling Challenge Matrix:\n",
    "\n",
    "                    â”‚ Small Model     â”‚ Large Model      â”‚ Production Scale  â”‚\n",
    "                    â”‚ (MNIST)         â”‚ (ImageNet)       â”‚ (GPT/BERT)        â”‚\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "Classes (C)         â”‚ 10              â”‚ 1,000            â”‚ 50,000+           â”‚\n",
    "Batch Size (B)      â”‚ 64              â”‚ 256              â”‚ 2,048             â”‚\n",
    "Memory (CE)         â”‚ 2.5 KB          â”‚ 1 MB             â”‚ 400 MB            â”‚\n",
    "Memory (MSE)        â”‚ 0.25 KB         â”‚ 1 KB             â”‚ 8 KB              â”‚\n",
    "Bottleneck          â”‚ None            â”‚ Softmax compute  â”‚ Vocabulary memory â”‚\n",
    "\n",
    "Memory grows as B*C for cross-entropy!\n",
    "At scale, vocabulary (C) dominates everything.\n",
    "```\n",
    "\n",
    "## ğŸ“Š Engineering Optimizations in Production\n",
    "\n",
    "```\n",
    "Common Production Optimizations:\n",
    "\n",
    "1. Hierarchical Softmax:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Full Softmax:       â”‚     â”‚ Hierarchical:       â”‚\n",
    "   â”‚ O(V) per sample     â”‚ â†’   â”‚ O(log V) per sample â”‚\n",
    "   â”‚ 50k classes = 50k   â”‚     â”‚ 50k classes = 16    â”‚\n",
    "   â”‚ operations          â”‚     â”‚ operations          â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "2. Sampled Softmax:\n",
    "   Instead of computing over all 50k classes,\n",
    "   sample 1k negative classes + correct class.\n",
    "   50Ã— speedup for training!\n",
    "\n",
    "3. Label Smoothing:\n",
    "   Instead of hard targets [0, 0, 1, 0],\n",
    "   use soft targets [0.1, 0.1, 0.7, 0.1].\n",
    "   Improves generalization.\n",
    "\n",
    "4. Mixed Precision:\n",
    "   Use FP16 for forward pass, FP32 for loss.\n",
    "   2Ã— memory reduction, same accuracy.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37790ecb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze_production_patterns",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_production_patterns():\n",
    "    \"\"\"\n",
    "    ğŸš€ Analyze loss function patterns in production ML systems.\n",
    "\n",
    "    Real insights from systems perspective.\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Production Analysis: Loss Function Engineering Patterns...\")\n",
    "\n",
    "    print(\"\\n1. Loss Function Choice by Problem Type:\")\n",
    "\n",
    "    scenarios = [\n",
    "        (\"Recommender Systems\", \"BCE/MSE\", \"User preference prediction\", \"Billions of interactions\"),\n",
    "        (\"Computer Vision\", \"CrossEntropy\", \"Image classification\", \"1000+ classes, large batches\"),\n",
    "        (\"NLP Translation\", \"CrossEntropy\", \"Next token prediction\", \"50k+ vocabulary\"),\n",
    "        (\"Medical Diagnosis\", \"BCE\", \"Disease probability\", \"Class imbalance critical\"),\n",
    "        (\"Financial Trading\", \"MSE/Huber\", \"Price prediction\", \"Outlier robustness needed\")\n",
    "    ]\n",
    "\n",
    "    print(\"System Type          | Loss Type    | Use Case              | Scale Challenge\")\n",
    "    print(\"-\" * 80)\n",
    "    for system, loss_type, use_case, challenge in scenarios:\n",
    "        print(f\"{system:20} | {loss_type:12} | {use_case:20} | {challenge}\")\n",
    "\n",
    "    print(\"\\n2. Engineering Trade-offs:\")\n",
    "\n",
    "    trade_offs = [\n",
    "        (\"CrossEntropy vs Label Smoothing\", \"Stability vs Confidence\", \"Label smoothing prevents overconfident predictions\"),\n",
    "        (\"MSE vs Huber Loss\", \"Sensitivity vs Robustness\", \"Huber is less sensitive to outliers\"),\n",
    "        (\"Full Softmax vs Sampled\", \"Accuracy vs Speed\", \"Hierarchical softmax for large vocabularies\"),\n",
    "        (\"Per-Sample vs Batch Loss\", \"Accuracy vs Memory\", \"Batch computation is more memory efficient\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTrade-off                    | Spectrum              | Production Decision\")\n",
    "    print(\"-\" * 85)\n",
    "    for trade_off, spectrum, decision in trade_offs:\n",
    "        print(f\"{trade_off:28} | {spectrum:20} | {decision}\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Production Insights:\")\n",
    "    print(\"   - Large vocabularies (50k+ tokens) dominate memory in CrossEntropy\")\n",
    "    print(\"   - Batch computation is 10-100Ã— more efficient than per-sample\")\n",
    "    print(\"   - Numerical stability becomes critical at scale (FP16 training)\")\n",
    "    print(\"   - Loss computation is often <5% of total training time\")\n",
    "\n",
    "# Run production analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_production_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4953a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ§ª Module Integration Test\n",
    "\n",
    "Final validation that everything works together correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0209be",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_module",
     "locked": true,
     "points": 20
    }
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"ğŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire losses module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_log_softmax()\n",
    "    test_unit_mse_loss()\n",
    "    test_unit_cross_entropy_loss()\n",
    "    test_unit_binary_cross_entropy_loss()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic end-to-end scenario with previous modules\n",
    "    print(\"ğŸ”¬ Integration Test: Realistic training scenario...\")\n",
    "\n",
    "    # Simulate a complete prediction -> loss computation pipeline\n",
    "\n",
    "    # 1. MSE for regression (house price prediction)\n",
    "    house_predictions = Tensor([250.0, 180.0, 320.0, 400.0])  # Predicted prices in thousands\n",
    "    house_actual = Tensor([245.0, 190.0, 310.0, 420.0])       # Actual prices\n",
    "    mse_loss = MSELoss()\n",
    "    house_loss = mse_loss.forward(house_predictions, house_actual)\n",
    "    assert house_loss.data > 0, \"House price loss should be positive\"\n",
    "    assert house_loss.data < 1000, \"House price loss should be reasonable\"\n",
    "\n",
    "    # 2. CrossEntropy for classification (image recognition)\n",
    "    image_logits = Tensor([[2.1, 0.5, 0.3], [0.2, 2.8, 0.1], [0.4, 0.3, 2.2]])  # 3 images, 3 classes\n",
    "    image_labels = Tensor([0, 1, 2])  # Correct class for each image\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    image_loss = ce_loss.forward(image_logits, image_labels)\n",
    "    assert image_loss.data > 0, \"Image classification loss should be positive\"\n",
    "    assert image_loss.data < 5.0, \"Image classification loss should be reasonable\"\n",
    "\n",
    "    # 3. BCE for binary classification (spam detection)\n",
    "    spam_probabilities = Tensor([0.85, 0.12, 0.78, 0.23, 0.91])\n",
    "    spam_labels = Tensor([1.0, 0.0, 1.0, 0.0, 1.0])  # True spam labels\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "    spam_loss = bce_loss.forward(spam_probabilities, spam_labels)\n",
    "    assert spam_loss.data > 0, \"Spam detection loss should be positive\"\n",
    "    assert spam_loss.data < 5.0, \"Spam detection loss should be reasonable\"\n",
    "\n",
    "    # 4. Test numerical stability with extreme values\n",
    "    extreme_logits = Tensor([[100.0, -100.0, 0.0]])\n",
    "    extreme_targets = Tensor([0])\n",
    "    extreme_loss = ce_loss.forward(extreme_logits, extreme_targets)\n",
    "    assert not np.isnan(extreme_loss.data), \"Loss should handle extreme values\"\n",
    "    assert not np.isinf(extreme_loss.data), \"Loss should not be infinite\"\n",
    "\n",
    "    print(\"âœ… End-to-end loss computation works!\")\n",
    "    print(\"âœ… All loss functions handle edge cases!\")\n",
    "    print(\"âœ… Numerical stability verified!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976cf76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run comprehensive module test\n",
    "if __name__ == \"__main__\":\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea683156",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¤” ML Systems Questions - Testing Your Understanding\n",
    "\n",
    "Before we finish, let's reflect on what you've learned about loss functions from a systems perspective.\n",
    "\n",
    "### Memory and Performance\n",
    "\n",
    "**Question 1: Loss Function Selection for Large Vocabulary**\n",
    "\n",
    "You're building a language model with a 50,000 word vocabulary. Your GPU has 16GB of memory, and you want to use batch size 128.\n",
    "\n",
    "Calculate:\n",
    "- How much memory does CrossEntropyLoss need for one forward pass? (Hint: B=128, C=50,000, float32)\n",
    "- If this exceeds your budget, what are three strategies to reduce memory usage?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "Memory for logits = Batch_Size Ã— Num_Classes Ã— 4 bytes (float32) = 128 Ã— 50,000 Ã— 4 = 25.6 MB\n",
    "\n",
    "For full forward pass with intermediate tensors (softmax, log_softmax), multiply by ~3 = 76.8 MB\n",
    "\n",
    "Strategies to reduce memory:\n",
    "1. **Sampled softmax**: Only compute softmax over subset of vocabulary (1000 samples)\n",
    "2. **Hierarchical softmax**: Use tree structure, O(log V) instead of O(V)\n",
    "3. **Mixed precision**: Use FP16 for forward pass (2 bytes instead of 4)\n",
    "4. **Gradient checkpointing**: Recompute intermediate activations instead of storing\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2: Loss Function Performance Bottleneck**\n",
    "\n",
    "You profile your training loop and find:\n",
    "- Forward pass (model): 80ms\n",
    "- Loss computation: 120ms\n",
    "- Backward pass: 150ms\n",
    "\n",
    "Your model has 1000 output classes. What's the bottleneck and how would you fix it?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "**Bottleneck**: Loss computation (120ms) taking longer than forward pass (80ms) is unusual.\n",
    "\n",
    "**Root Cause**: Softmax computation in CrossEntropyLoss is O(BÃ—C). With C=1000, this dominates.\n",
    "\n",
    "**Solutions**:\n",
    "1. **Hierarchical softmax**: Reduces complexity from O(C) to O(log C)\n",
    "2. **Sampled softmax**: Only compute over subset of classes during training\n",
    "3. **Optimize softmax kernel**: Use fused operations (PyTorch does this automatically)\n",
    "4. **Check batch size**: Very small batches don't utilize GPU well\n",
    "\n",
    "**Reality Check**: In well-optimized PyTorch, loss should be ~5-10% of training time, not 35%!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "**Question 3: Debugging Exploding Loss**\n",
    "\n",
    "During training, you see:\n",
    "```\n",
    "Epoch 1: Loss = 2.3\n",
    "Epoch 2: Loss = 1.8\n",
    "Epoch 3: Loss = inf\n",
    "```\n",
    "\n",
    "The model uses CrossEntropyLoss with raw logits reaching values like [150, -80, 200].\n",
    "\n",
    "Why did loss become infinite? What code change fixes this?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "**Root Cause**: Without the log-sum-exp trick, computing softmax directly causes:\n",
    "```python\n",
    "exp(200) = 7.2 Ã— 10^86  # Overflows to infinity in float32\n",
    "```\n",
    "\n",
    "**The Fix**: Use log_softmax with max subtraction (already implemented in your code!):\n",
    "```python\n",
    "# âŒ Naive approach (causes overflow)\n",
    "softmax = np.exp(logits) / np.sum(np.exp(logits))\n",
    "loss = -np.log(softmax[target])\n",
    "\n",
    "# âœ… Stable approach (your implementation)\n",
    "log_softmax = logits - np.max(logits) - np.log(np.sum(np.exp(logits - np.max(logits))))\n",
    "loss = -log_softmax[target]\n",
    "```\n",
    "\n",
    "**Verification**: Your `log_softmax()` function handles this automatically. Check that you're using it in `CrossEntropyLoss.forward()`.\n",
    "\n",
    "**Prevention**: Always use log-space computations for probabilities!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Question 4: Real-Time Inference Latency**\n",
    "\n",
    "Your spam filter needs to classify emails in <10ms. Currently:\n",
    "- Model inference: 3ms\n",
    "- Loss computation: 8ms (â“ Why are we computing loss?)\n",
    "\n",
    "Your inference code looks like:\n",
    "```python\n",
    "prediction = model(email)\n",
    "confidence = bce_loss(prediction, threshold)  # Using loss for confidence?\n",
    "```\n",
    "\n",
    "What's wrong with this approach, and how would you fix it?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "**Critical Mistake**: Loss functions are for **training**, not **inference**!\n",
    "\n",
    "**Why it's wrong**:\n",
    "- Loss requires ground truth labels (not available at inference time)\n",
    "- Loss computation adds unnecessary overhead\n",
    "- You already have the prediction probability!\n",
    "\n",
    "**Correct inference code**:\n",
    "```python\n",
    "prediction = model(email)  # Returns probability between 0 and 1\n",
    "is_spam = prediction.data > 0.5  # Simple threshold\n",
    "\n",
    "# If you need confidence score:\n",
    "confidence = abs(prediction.data - 0.5) * 2  # Distance from decision boundary\n",
    "# Or just use the raw probability: prediction.data\n",
    "```\n",
    "\n",
    "**Performance gain**: 3ms (73% faster!) just by removing unnecessary loss computation.\n",
    "\n",
    "**Key insight**: Loss functions measure \"wrongness\" during training. At inference, you already have the model's output - use it directly!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 5: Class Imbalance in Medical Diagnosis**\n",
    "\n",
    "You're building a cancer detection system:\n",
    "- 95% of samples are negative (healthy)\n",
    "- 5% are positive (cancer)\n",
    "\n",
    "Using vanilla BinaryCrossEntropyLoss, your model achieves 95% accuracy by always predicting \"healthy.\"\n",
    "\n",
    "What are three ways to handle this with loss functions?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "**The Problem**: Model learned to exploit class imbalance - always predict majority class!\n",
    "\n",
    "**Solution 1: Weighted Loss**\n",
    "```python\n",
    "class WeightedBCELoss:\n",
    "    def __init__(self, pos_weight=19.0):  # 95/5 = 19\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = -(self.pos_weight * target * np.log(pred) +\n",
    "                 (1-target) * np.log(1-pred))\n",
    "        return np.mean(loss)\n",
    "```\n",
    "Penalize missed cancer cases 19Ã— more than false alarms.\n",
    "\n",
    "**Solution 2: Focal Loss**\n",
    "```python\n",
    "# Focuses on hard examples (misclassified samples)\n",
    "focal_loss = -(1 - p_correct)^gamma * log(p_correct)\n",
    "```\n",
    "Automatically downweights easy examples (majority class).\n",
    "\n",
    "**Solution 3: Resampling**\n",
    "- Oversample minority class (duplicate cancer cases)\n",
    "- Undersample majority class (fewer healthy samples)\n",
    "- SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "**Medical Reality**: Weighted loss is most common. False negatives (missed cancer) are MUCH worse than false positives (unnecessary tests).\n",
    "\n",
    "**Critical Insight**: 95% accuracy is meaningless! Track precision, recall, F1, and AUC instead.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Systems Thinking\n",
    "\n",
    "**Question 6: Batch Size and Loss Computation**\n",
    "\n",
    "You're training on a GPU with 24GB memory. With batch size 32, memory usage is 8GB. You increase batch size to 128.\n",
    "\n",
    "Will memory usage be 32GB (4Ã— increase)? Why or why not?\n",
    "\n",
    "What happens to:\n",
    "- Loss computation time?\n",
    "- Loss value (the actual number)?\n",
    "- Gradient quality?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Hint</summary>\n",
    "\n",
    "**Memory Usage**: YES, approximately 32GB (4Ã— increase) - **EXCEEDS GPU MEMORY! Training will crash.**\n",
    "\n",
    "**Why linear scaling?**\n",
    "```\n",
    "Memory = Model_Params + Batch_Size Ã— (Activations + Gradients + Optimizer_State)\n",
    "         â†‘              â†‘\n",
    "      Fixed (1GB)     Scales linearly (7GB â†’ 28GB)\n",
    "```\n",
    "\n",
    "**Loss computation time**: ~4Ã— slower (linear with batch size)\n",
    "- 32 samples: 0.5ms\n",
    "- 128 samples: 2.0ms\n",
    "\n",
    "**Loss value**: **SAME** (we take mean over batch)\n",
    "```python\n",
    "# Both compute the same thing:\n",
    "batch_32_loss = np.mean(losses[:32])   # Mean of 32 samples\n",
    "batch_128_loss = np.mean(losses[:128]) # Mean of 128 samples\n",
    "```\n",
    "\n",
    "**Gradient quality**: **BETTER** - larger batch = more stable gradient estimate\n",
    "- Batch 32: High variance, noisy gradients\n",
    "- Batch 128: Lower variance, smoother convergence\n",
    "\n",
    "**The Trade-off**:\n",
    "- Larger batch = better gradients but more memory\n",
    "- Smaller batch = less memory but noisier training\n",
    "- Sweet spot: Usually 64-256 depending on GPU memory\n",
    "\n",
    "**Production Solution**: Gradient accumulation\n",
    "```python\n",
    "# Simulate batch_size=128 with only batch_size=32 memory:\n",
    "for micro_batch in range(4):  # 4 Ã— 32 = 128\n",
    "    loss = compute_loss(micro_batch)\n",
    "    loss.backward()  # Accumulate gradients\n",
    "optimizer.step()  # Update once with accumulated gradients\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "These questions test your systems understanding of loss functions - not just \"how do they work\" but \"how do they behave in production at scale.\" Keep these considerations in mind as you build real ML systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2159e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## â­ Aha Moment: Loss Guides Learning\n",
    "\n",
    "**What you built:** Loss functions that measure how wrong predictions are.\n",
    "\n",
    "**Why it matters:** Without loss, there's no learning. The loss function is the \"coach\"\n",
    "that tells the network whether its predictions are good or bad. Lower loss = better\n",
    "predictions. Every training step aims to reduce this number.\n",
    "\n",
    "In the next module, you'll add autograd which computes gradients of this lossâ€”the\n",
    "direction to adjust weights to make predictions better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f36f8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_losses():\n",
    "    \"\"\"ğŸ¯ See how loss responds to prediction quality.\"\"\"\n",
    "    print(\"ğŸ¯ AHA MOMENT: Loss Guides Learning\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    loss_fn = MSELoss()\n",
    "    target = Tensor(np.array([1.0, 0.0, 0.0]))\n",
    "\n",
    "    # Perfect prediction\n",
    "    perfect = Tensor(np.array([1.0, 0.0, 0.0]))\n",
    "    loss_perfect = loss_fn(perfect, target)\n",
    "\n",
    "    # Close prediction\n",
    "    close = Tensor(np.array([0.9, 0.1, 0.1]))\n",
    "    loss_close = loss_fn(close, target)\n",
    "\n",
    "    # Wrong prediction\n",
    "    wrong = Tensor(np.array([0.0, 1.0, 1.0]))\n",
    "    loss_wrong = loss_fn(wrong, target)\n",
    "\n",
    "    print(f\"Perfect prediction â†’ Loss: {float(loss_perfect.data):.4f}\")\n",
    "    print(f\"Close prediction   â†’ Loss: {float(loss_close.data):.4f}\")\n",
    "    print(f\"Wrong prediction   â†’ Loss: {float(loss_wrong.data):.4f}\")\n",
    "\n",
    "    print(\"\\nâœ¨ Lower loss = better predictions! Training minimizes this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_module()\n",
    "    print(\"\\n\")\n",
    "    demo_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d00df7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸš€ MODULE SUMMARY: Losses\n",
    "\n",
    "Congratulations! You've built the measurement system that enables all machine learning!\n",
    "\n",
    "### Key Accomplishments\n",
    "- Built 3 essential loss functions: MSE, CrossEntropy, and BinaryCrossEntropy âœ…\n",
    "- Implemented numerical stability with log-sum-exp trick âœ…\n",
    "- Discovered memory scaling patterns with batch size and vocabulary âœ…\n",
    "- Analyzed production trade-offs between different loss function choices âœ…\n",
    "- All tests pass âœ… (validated by `test_module()`)\n",
    "\n",
    "### Ready for Next Steps\n",
    "Your loss functions provide the essential feedback signal for learning. These \"error measurements\" will become the starting point for backpropagation in Module 06 (Autograd)!\n",
    "Export with: `tito module complete 04`\n",
    "\n",
    "**Next**: Module 05 will add DataLoader for efficient data pipelines, then Module 06 adds automatic differentiation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
